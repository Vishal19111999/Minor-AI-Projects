{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = data.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = data.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k:(v+3) for k,v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i,\"?\") for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 250)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model.add(keras.layers.Embedding(10000,16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16,activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 1s 97us/sample - loss: 0.6920 - acc: 0.5259 - val_loss: 0.6904 - val_acc: 0.5609\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.6871 - acc: 0.6495 - val_loss: 0.6836 - val_acc: 0.6747\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.6754 - acc: 0.7245 - val_loss: 0.6683 - val_acc: 0.7473\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.6547 - acc: 0.7506 - val_loss: 0.6449 - val_acc: 0.7633\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.6247 - acc: 0.7855 - val_loss: 0.6138 - val_acc: 0.7856\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.5869 - acc: 0.8087 - val_loss: 0.5771 - val_acc: 0.7969\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.5442 - acc: 0.8229 - val_loss: 0.5367 - val_acc: 0.8165\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.5005 - acc: 0.8418 - val_loss: 0.4982 - val_acc: 0.8269\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 1s 45us/sample - loss: 0.4586 - acc: 0.8543 - val_loss: 0.4622 - val_acc: 0.8386\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.4203 - acc: 0.8665 - val_loss: 0.4313 - val_acc: 0.8455\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.3871 - acc: 0.8760 - val_loss: 0.4049 - val_acc: 0.8543\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.3585 - acc: 0.8849 - val_loss: 0.3844 - val_acc: 0.8577\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.3349 - acc: 0.8901 - val_loss: 0.3653 - val_acc: 0.8632\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.3133 - acc: 0.8967 - val_loss: 0.3513 - val_acc: 0.8679\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 47us/sample - loss: 0.2955 - acc: 0.8993 - val_loss: 0.3393 - val_acc: 0.8700\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 52us/sample - loss: 0.2793 - acc: 0.9056 - val_loss: 0.3294 - val_acc: 0.8719\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.2647 - acc: 0.9099 - val_loss: 0.3211 - val_acc: 0.8751\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 46us/sample - loss: 0.2516 - acc: 0.9144 - val_loss: 0.3137 - val_acc: 0.8777\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.2400 - acc: 0.9177 - val_loss: 0.3076 - val_acc: 0.8792\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.2293 - acc: 0.9210 - val_loss: 0.3034 - val_acc: 0.8807\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.2189 - acc: 0.9251 - val_loss: 0.2995 - val_acc: 0.8814\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 1s 48us/sample - loss: 0.2099 - acc: 0.9287 - val_loss: 0.2954 - val_acc: 0.8820\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.2009 - acc: 0.9315 - val_loss: 0.2936 - val_acc: 0.8822\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 49us/sample - loss: 0.1931 - acc: 0.9349 - val_loss: 0.2909 - val_acc: 0.8834\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 56us/sample - loss: 0.1851 - acc: 0.9398 - val_loss: 0.2884 - val_acc: 0.8840\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 54us/sample - loss: 0.1780 - acc: 0.9425 - val_loss: 0.2880 - val_acc: 0.8842\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.1713 - acc: 0.9457 - val_loss: 0.2866 - val_acc: 0.8842\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 1s 55us/sample - loss: 0.1648 - acc: 0.9485 - val_loss: 0.2861 - val_acc: 0.8852\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 1s 41us/sample - loss: 0.1593 - acc: 0.9503 - val_loss: 0.2865 - val_acc: 0.8842\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.1534 - acc: 0.9516 - val_loss: 0.2854 - val_acc: 0.8862\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 43us/sample - loss: 0.1473 - acc: 0.9561 - val_loss: 0.2856 - val_acc: 0.8860\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.1420 - acc: 0.9578 - val_loss: 0.2861 - val_acc: 0.8863\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 1s 58us/sample - loss: 0.1367 - acc: 0.9597 - val_loss: 0.2875 - val_acc: 0.8859\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 1s 66us/sample - loss: 0.1321 - acc: 0.9617 - val_loss: 0.2885 - val_acc: 0.8871\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 1s 56us/sample - loss: 0.1280 - acc: 0.9623 - val_loss: 0.2896 - val_acc: 0.8865\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 1s 53us/sample - loss: 0.1231 - acc: 0.9650 - val_loss: 0.2912 - val_acc: 0.8858\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 1s 51us/sample - loss: 0.1186 - acc: 0.9669 - val_loss: 0.2930 - val_acc: 0.8867\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 1s 61us/sample - loss: 0.1145 - acc: 0.9677 - val_loss: 0.2955 - val_acc: 0.8857\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 1s 59us/sample - loss: 0.1111 - acc: 0.9685 - val_loss: 0.2980 - val_acc: 0.8849\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 1s 61us/sample - loss: 0.1070 - acc: 0.9711 - val_loss: 0.2995 - val_acc: 0.8856\n",
      "25000/25000 [==============================] - 1s 33us/sample - loss: 0.3196 - acc: 0.8718\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]\n",
    "y_val = train_labels[:10000]\n",
    "y_train = train_labels[10000:]\n",
    "\n",
    "fitmodel = model.fit(x_train, y_train, epochs = 40, batch_size = 512, validation_data = (x_val, y_val), verbose = 1)\n",
    "\n",
    "results = model.evaluate(test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31961071040153505, 0.8718]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      "king irritated <UNK> sends them a <UNK> br br delighted with this <UNK> looking new king who towers above them the <UNK> welcome him with a <UNK> of <UNK> dressed <UNK> the mayor steps forward to hand him the key to the <UNK> as <UNK> cameras record the event to everyone's horror the <UNK> promptly eats the mayor and then goes on a merry rampage <UNK> citizens at random a title card <UNK> reads news of the king's <UNK> throughout the kingdom when the now terrified <UNK> once more <UNK> <UNK> for help he loses his temper and <UNK> their community with lightning <UNK> the moral of our story delivered by a hapless frog just before he is eaten is let well enough alone br br considering the time period when this startling little film was made and considering the fact that it was made by a russian <UNK> at the height of that <UNK> country's civil war it would be easy to see this as a <UNK> about those events <UNK> may or may not have had <UNK> turmoil in mind when he made <UNK> but whatever <UNK> his choice of material the film stands as a <UNK> tale of universal <UNK> <UNK> could be the soviet union italy germany or japan in the 1930s or any country of any era that lets its guard down and is overwhelmed by <UNK> it's a fascinating film even a charming one in its macabre way but its message is no joke\n",
      "Prediction: [0.00049433]\n",
      "Actual: 1\n",
      "[0.31961071040153505, 0.8718]\n"
     ]
    }
   ],
   "source": [
    "test_review = test_data[2]\n",
    "predict = model.predict([test_review])\n",
    "print(\"Review: \")\n",
    "print(decode_review(test_review))\n",
    "print(\"Prediction: \" + str(predict[2]))\n",
    "print(\"Actual: \" + str(test_labels[2]))\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
